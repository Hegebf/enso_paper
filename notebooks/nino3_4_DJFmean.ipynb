{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Nino3.4 DJF index for each model, and save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "from scipy.signal import detrend\n",
    "from matplotlib import pyplot as plt\n",
    "from eofs.xarray import Eof\n",
    "from scipy import signal\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import xesmf as xe\n",
    "import intake\n",
    "import pprint \n",
    "import util \n",
    "\n",
    "if util.is_ncar_host():\n",
    "    col = intake.open_esm_datastore(\"../catalogs/glade-cmip6.json\")\n",
    "else:\n",
    "    col = intake.open_esm_datastore(\"../catalogs/pangeo-cmip6.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['BCC-CSM2-MR', 'FGOALS-g3', 'CanESM5', 'CNRM-CM6-1', 'CNRM-ESM2-1',\n",
      "       'E3SM-1-0', 'EC-Earth3', 'EC-Earth3-Veg', 'IPSL-CM6A-LR', 'MIROC-ES2L',\n",
      "       'MIROC6', 'UKESM1-0-LL', 'MRI-ESM2-0', 'GISS-E2-1-G', 'GISS-E2-1-H',\n",
      "       'CESM2', 'CESM2-WACCM', 'GFDL-CM4', 'SAM0-UNICON'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# pick only models with at least 496 yrs in piControl\n",
    "minyrs_control = 496;\n",
    "# models with fewer years often missed future scenarios, so they are not so interesting for us\n",
    "\n",
    "# load table:\n",
    "data_table = pd.read_table('available_data.txt',index_col=0)\n",
    "models_used = data_table['piControl (yrs)'][data_table['piControl (yrs)'] >= minyrs_control].index\n",
    "print(models_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BCC-CSM2-MR'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models_used[0]\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piControl (ens.mem.)         1\n",
       "historical (ens.mem.)        3\n",
       "ssp126 (ens.mem.)          NaN\n",
       "ssp245 (ens.mem.)            1\n",
       "ssp370 (ens.mem.)          NaN\n",
       "ssp585 (ens.mem.)            1\n",
       "abrupt-4xCO2 (ens.mem.)      1\n",
       "piControl (yrs)            600\n",
       "historical (yrs)           165\n",
       "ssp126 (yrs)               NaN\n",
       "ssp245 (yrs)                86\n",
       "ssp370 (yrs)               NaN\n",
       "ssp585 (yrs)                86\n",
       "abrupt-4xCO2 (yrs)         151\n",
       "Name: BCC-CSM2-MR, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table.loc[model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['piControl', 'historical', 'ssp245', 'ssp585']\n"
     ]
    }
   ],
   "source": [
    "# what experiments does this model have that we want to study?\n",
    "exp_list = [exp[:-11] for exp in data_table.loc[model][:6].index if float(data_table.loc[model][:6][exp]) > 0]\n",
    "print(exp_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "piControl\n",
      "\n",
      "xarray will load netCDF datasets with dask using a single chunk for all arrays.\n",
      "For effective chunking, please provide chunks in cdf_kwargs.\n",
      "For example: cdf_kwargs={'chunks': {'time': 36}}\n",
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "historical\n",
      "\n",
      "xarray will load netCDF datasets with dask using a single chunk for all arrays.\n",
      "For effective chunking, please provide chunks in cdf_kwargs.\n",
      "For example: cdf_kwargs={'chunks': {'time': 36}}\n",
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "ssp245\n",
      "\n",
      "xarray will load netCDF datasets with dask using a single chunk for all arrays.\n",
      "For effective chunking, please provide chunks in cdf_kwargs.\n",
      "For example: cdf_kwargs={'chunks': {'time': 36}}\n",
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "ssp585\n",
      "\n",
      "xarray will load netCDF datasets with dask using a single chunk for all arrays.\n",
      "For effective chunking, please provide chunks in cdf_kwargs.\n",
      "For example: cdf_kwargs={'chunks': {'time': 36}}\n",
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'piControl': 'CMIP.BCC.BCC-CSM2-MR.piControl.Amon.gn',\n",
       " 'historical': 'CMIP.BCC.BCC-CSM2-MR.historical.Amon.gn',\n",
       " 'ssp245': 'ScenarioMIP.BCC.BCC-CSM2-MR.ssp245.Amon.gn',\n",
       " 'ssp585': 'ScenarioMIP.BCC.BCC-CSM2-MR.ssp585.Amon.gn'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_keys = {}; datasets = {}\n",
    "\n",
    "for exp in exp_list:\n",
    "    print(exp)\n",
    "    cat = col.search(experiment_id = exp, source_id = model, variable_id='ts', table_id='Amon')\n",
    "        \n",
    "    dset_dict = cat.to_dataset_dict(zarr_kwargs={'consolidated': True}, cdf_kwargs={'chunks': {}})\n",
    "    for key in dset_dict.keys():\n",
    "        exp_keys[exp] = key\n",
    "        datasets[key] = dset_dict[key]\n",
    "\n",
    "exp_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def area_weights(lat_bnds, lon_bnds): \n",
    "    # computes exact area weigths assuming earth is a perfect sphere\n",
    "    lowerlats = np.radians(lat_bnds[:,0]); upperlats = np.radians(lat_bnds[:,1])\n",
    "    difflon = np.radians(np.diff(lon_bnds[0,:])) # if the differences in longitudes are all the same\n",
    "    areaweights = difflon*(np.sin(upperlats) - np.sin(lowerlats));\n",
    "    areaweights /= areaweights.mean()\n",
    "    return areaweights # list of weights, of same dimension as latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# days per month:\n",
    "dpm = {'noleap': [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]}\n",
    "\n",
    "def day_weights(years, chosen_season = 'DJF', calendar = 'noleap'):\n",
    "    \n",
    "    if chosen_season == 'DJF':\n",
    "        dpm_season = [dpm[calendar][-1]] + dpm[calendar][:2]\n",
    "    elif chosen_season == 'all':\n",
    "        dpm_season = dpm[calendar]\n",
    "    # if other season wanted, continue developing this if-test\n",
    "    \n",
    "    # normalise weights within relevant season to have mean 1\n",
    "    # NB: does not care what numbers are produced for other seasons\n",
    "    norm_season = np.mean(dpm_season)\n",
    "    weights = np.tile(np.array(dpm[calendar]),years)/norm_season\n",
    "    return weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "historical\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'time' (time: 1980)>\n",
       "array([cftime.DatetimeNoLeap(1850, 1, 16, 12, 0, 0, 0, 3, 16),\n",
       "       cftime.DatetimeNoLeap(1850, 2, 15, 0, 0, 0, 0, 5, 46),\n",
       "       cftime.DatetimeNoLeap(1850, 3, 16, 12, 0, 0, 0, 6, 75), ...,\n",
       "       cftime.DatetimeNoLeap(2014, 10, 16, 12, 0, 0, 0, 6, 289),\n",
       "       cftime.DatetimeNoLeap(2014, 11, 16, 0, 0, 0, 0, 2, 320),\n",
       "       cftime.DatetimeNoLeap(2014, 12, 16, 12, 0, 0, 0, 4, 350)], dtype=object)\n",
       "Coordinates:\n",
       "    member_id  <U8 'r1i1p1f1'\n",
       "  * time       (time) object 1850-01-16 12:00:00 ... 2014-12-16 12:00:00\n",
       "Attributes:\n",
       "    bounds:         time_bnds\n",
       "    axis:           T\n",
       "    long_name:      time\n",
       "    standard_name:  time"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load a dataset for calendar check:\n",
    "# if other than noleap, above function must be changed\n",
    "exp = exp_list[1]; print(exp)\n",
    "key = exp_keys[exp]\n",
    "exp_datasets = datasets[key]\n",
    "members_sorted = exp_datasets.member_id.sortby(exp_datasets.member_id)\n",
    "\n",
    "ds = exp_datasets.sel(member_id = members_sorted[0])\n",
    "ds.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2439.0\n",
      "branch time unit is likely years\n"
     ]
    }
   ],
   "source": [
    "# do a manual check of the time unit of the branch time,\n",
    "# since it is not specified if the unit is years or days\n",
    "print(ds.branch_time_in_parent)\n",
    "if ds.branch_time_in_parent % 365 == 0:\n",
    "    print('branch time unit is likely', ds.parent_time_units)\n",
    "    branch_time_unit = 'days'\n",
    "else:\n",
    "    #print(ds.branch_time_in_parent % 365)\n",
    "    print('branch time unit is likely years')\n",
    "    branch_time_unit = 'years'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "piControl r1i1p1f1\n",
      "Reuse existing file: bilinear_18x53_10x50.nc\n",
      "historical r1i1p1f1\n",
      "Reuse existing file: bilinear_18x53_10x50.nc\n",
      "historical r2i1p1f1\n",
      "Reuse existing file: bilinear_18x53_10x50.nc\n",
      "historical r3i1p1f1\n",
      "Reuse existing file: bilinear_18x53_10x50.nc\n",
      "ssp245 r1i1p1f1\n",
      "Reuse existing file: bilinear_18x53_10x50.nc\n",
      "ssp585 r1i1p1f1\n",
      "Reuse existing file: bilinear_18x53_10x50.nc\n"
     ]
    }
   ],
   "source": [
    "latregion = slice(-5,5); lonregion = slice(190, 240) # = 120 W - 170 W\n",
    "# use larger region before regridding, that adds 5 deg to each border:\n",
    "larger_latregion = slice(-10,10); larger_lonregion = slice(185, 245)\n",
    "resolution = 1;\n",
    "ds_out = xr.Dataset({'lon': (['lon'], np.arange(lonregion.start+resolution/2, lonregion.stop+resolution/2, resolution)),\n",
    "                     'lat': (['lat'], np.arange(latregion.start+resolution/2, latregion.stop+resolution/2, resolution))\n",
    "                    }\n",
    "                   )\n",
    "regr_lat_bnds = np.array([[upper, upper+resolution] for upper in range(latregion.start,latregion.stop)])\n",
    "regr_lon_bnds = np.array([[upper, upper+resolution] for upper in range(lonregion.start,lonregion.stop)])\n",
    "area_w = area_weights(regr_lat_bnds, regr_lon_bnds)\n",
    "\n",
    "season = 'DJF'\n",
    "y_ext = 250; # years between 1850 to 2100 \n",
    "branchyears = {}\n",
    "lastD = {}\n",
    "\n",
    "for exp in exp_list:\n",
    "    key = exp_keys[exp]\n",
    "    exp_datasets = datasets[key]\n",
    "    members_sorted = exp_datasets.member_id.sortby(exp_datasets.member_id)\n",
    "    branchyears[exp] = {};\n",
    "    for member in members_sorted.values:\n",
    "        print(exp, member)\n",
    "        ds = exp_datasets.sel(member_id = member)\n",
    "        \n",
    "        if ds.parent_experiment_id == 'historical':\n",
    "            # this should always have unit years\n",
    "            parent_branch_from_grandparent = branchyears['historical'][ds.parent_variant_label]\n",
    "            child_branch_from_grandparent = parent_branch_from_grandparent + data_table.loc[model, 'historical (yrs)']\n",
    "            branchyear = child_branch_from_grandparent\n",
    "        else:\n",
    "            if branch_time_unit == 'years':\n",
    "                branchyear = ds.branch_time_in_parent;\n",
    "            elif branch_time_unit == 'days':\n",
    "                branchyear = ds.branch_time_in_parent / 365;\n",
    "        branchyears[exp][member] = branchyear   \n",
    "        \n",
    "        # select regional data, perform a regridding, and compute area average\n",
    "        regional_data = ds.ts.sel(lat = larger_latregion, lon = larger_lonregion)\n",
    "        regridder = xe.Regridder(regional_data, ds_out, 'bilinear', reuse_weights = True)\n",
    "        regridded_data = regridder(regional_data)\n",
    "        area_avg = (regridded_data.transpose('time', 'lon', 'lat') * area_w).mean(dim=['lon', 'lat'])\n",
    "        \n",
    "        yrs = int(area_avg.shape[0]/12)\n",
    "\n",
    "        # average over season\n",
    "        day_weighted_avg = area_avg*day_weights(yrs, chosen_season = season)\n",
    "        ds_season = day_weighted_avg.where(day_weighted_avg['time.season'] == season) # creates nan in all other months\n",
    "        \n",
    "        if exp == 'historical':\n",
    "            # save last december month for each member for use in season mean in first year of ssp exps\n",
    "            lastD[member] = day_weighted_avg[-1]\n",
    "        elif exp not in ['piControl','historical']: # then it must be future scenario   \n",
    "            ds_season = xr.concat([lastD[member], ds_season], dim = 'time')\n",
    "            \n",
    "        ds_season = ds_season.rolling(min_periods=3, center=True, time=3).mean()\n",
    "        \n",
    "        if exp not in ['piControl','historical']:\n",
    "            # remove nan-value obtained from inserting last december month from historical\n",
    "            ds_season = ds_season[1:]\n",
    "        seasonmean = ds_season.groupby('time.year').mean('time') # make annual mean\n",
    "        # no information the first year of piControl and historical, since we are missing the december month before\n",
    "        \n",
    "        # day-weighted rolling 3-months mean for all months (with seasonal variations)\n",
    "        #day_weighted_avg_allyear = area_avg*day_weights(yrs, chosen_season = 'all')\n",
    "        #smoothed_allyear = day_weighted_avg_allyear.rolling(min_periods=3, center=True, time=3).mean()\n",
    "        \n",
    "        colname = [(exp, member)]\n",
    "        if exp == 'piControl' and member == 'r1i1p1f1':\n",
    "            # create dataframe for storing all results and make the piControl years the index\n",
    "            # extend years by y_ext years, in case historical branches in the end of piControl\n",
    "            control_years = seasonmean.year.values\n",
    "            years = list(control_years) + list(np.arange(control_years[-1]+1, control_years[-1] + y_ext+1))\n",
    "            data = np.append(seasonmean.values, np.full(y_ext, np.nan)) # append nans\n",
    "            df = pd.DataFrame(data, index = years, columns = colname)\n",
    "        else:\n",
    "            nans1 = np.full(int(branchyear - control_years[0]), np.nan);\n",
    "            nans2 = np.full(len(years) - len(seasonmean.values) - len(nans1), np.nan)\n",
    "            data = np.concatenate((nans1, seasonmean.values, nans2))\n",
    "\n",
    "            df_col = pd.DataFrame(data, index = years, columns = colname)\n",
    "            df = pd.merge(df, df_col, left_index=True, right_index=True, how='outer')\n",
    "            # all experiments are stored in piControl time units\n",
    "            # starting at their branchyear in piControl\n",
    "            # all other values are set to be nan\n",
    "        \n",
    "df.columns = pd.MultiIndex.from_tuples(df.columns, names=['Experiment','Member'])\n",
    "\n",
    "# remove all rows at the end containg only nan values\n",
    "allnans = True\n",
    "while allnans == True:\n",
    "    if all(np.isnan(df.iloc[-1])):\n",
    "        df = df[:-1]\n",
    "    else:\n",
    "        allnans = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Experiment</th>\n",
       "      <th>piControl</th>\n",
       "      <th colspan=\"3\" halign=\"left\">historical</th>\n",
       "      <th>ssp245</th>\n",
       "      <th>ssp585</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Member</th>\n",
       "      <th>r1i1p1f1</th>\n",
       "      <th>r1i1p1f1</th>\n",
       "      <th>r2i1p1f1</th>\n",
       "      <th>r3i1p1f1</th>\n",
       "      <th>r1i1p1f1</th>\n",
       "      <th>r1i1p1f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1851</td>\n",
       "      <td>300.445907</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1852</td>\n",
       "      <td>299.368412</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1853</td>\n",
       "      <td>298.780820</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1854</td>\n",
       "      <td>300.150619</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2685</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>301.273165</td>\n",
       "      <td>303.678246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2686</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>302.711977</td>\n",
       "      <td>302.845171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2687</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>301.885820</td>\n",
       "      <td>303.102484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2688</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>302.276828</td>\n",
       "      <td>303.891943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2689</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300.812118</td>\n",
       "      <td>303.547983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>840 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Experiment   piControl historical                        ssp245      ssp585\n",
       "Member        r1i1p1f1   r1i1p1f1 r2i1p1f1 r3i1p1f1    r1i1p1f1    r1i1p1f1\n",
       "1850               NaN        NaN      NaN      NaN         NaN         NaN\n",
       "1851        300.445907        NaN      NaN      NaN         NaN         NaN\n",
       "1852        299.368412        NaN      NaN      NaN         NaN         NaN\n",
       "1853        298.780820        NaN      NaN      NaN         NaN         NaN\n",
       "1854        300.150619        NaN      NaN      NaN         NaN         NaN\n",
       "...                ...        ...      ...      ...         ...         ...\n",
       "2685               NaN        NaN      NaN      NaN  301.273165  303.678246\n",
       "2686               NaN        NaN      NaN      NaN  302.711977  302.845171\n",
       "2687               NaN        NaN      NaN      NaN  301.885820  303.102484\n",
       "2688               NaN        NaN      NaN      NaN  302.276828  303.891943\n",
       "2689               NaN        NaN      NaN      NaN  300.812118  303.547983\n",
       "\n",
       "[840 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('../Processed_data/Nino3_4_DJF/' + model + '_DJF_nino3_4index.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CMIP6 2019.10a",
   "language": "python",
   "name": "cmip6-201910a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
